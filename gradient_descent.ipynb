{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18f07bc-92ff-4b44-922d-e34390bd2d10",
   "metadata": {},
   "source": [
    "# Gradient Descent Methods\n",
    "\n",
    "This tour explores the use of gradient descent method for unconstrained and constrained optimization of a smooth function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20caedcc-cad6-42b1-aaa4-790f8b171533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233b3567-6359-41cd-bffc-188e61ef2b17",
   "metadata": {},
   "source": [
    "We consider the problem of finding a minimum of a function $f$, hence solving\n",
    "$$(\\mathcal{P}): \\min_{x \\in \\mathbb R^d} f(x)$$\n",
    "where $f : \\mathbb R^d \\rightarrow \\mathbb R$ is a smooth function.\n",
    "\n",
    "Note that the minimum is not necessarily unique.\n",
    "In the general case, $f$ might exhibit local minima, in which case the proposed algorithms is not expected to find a global minimizer of the problem.\n",
    "In this tour, we restrict our attention to convex function, so that the methods will converge to a global minimizer.\n",
    "\n",
    "The simplest method is the gradient descent, that computes\n",
    "$$ x^{(k+1)} = x^{(k)} - \\tau_k \\nabla f(x^{(k)}), $$\n",
    "where $\\tau_k>0$ is a step size, and $\\nabla f(x) \\in \\mathbb R^d$ is the gradient of $f$ at the point $x$, and $x^{(0)} \\in \\mathbb R^d$ is any initial point.\n",
    "\n",
    "In the convex case, if $f$ is of class $C^2$, in order to ensure convergence, the step size should satisfy\n",
    "$$ 0 < \\tau_k < \\frac{2}{ \\sup_x \\|Hf(x)\\| } $$\n",
    "where $Hf(x) \\in \\mathbb R^{d \\times d}$ is the Hessian of $f$ at $x$ and $\\|\\cdot\\|$ is the spectral operator norm (largest eigenvalue)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6260b85-9bcd-4b86-a897-3e48aff98f96",
   "metadata": {},
   "source": [
    "## Exersice : Gradient Descent in 2-D\n",
    "\n",
    "We consider a simple problem, corresponding to the minimization of a 2-D quadratic form\n",
    "$$ f(x) = \\frac{1}{2} ( x_1^2 + \\eta x_2^2 ) ,\\quad x = (x_1, x_2)$$\n",
    "where $\\eta>0$ controls the anisotropy, and hence the difficulty, of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e36c14",
   "metadata": {},
   "source": [
    "# Part I (Theoretical analysis)\n",
    "1. Find a matrix $A$ such that $f(x)=\\frac{1}{2}\\langle Ax,x\\rangle$\n",
    "2. Find $\\eta$ such that the problem $(\\mathcal{P}):\\min_{x \\in \\mathbb{R}^2} f(x)$ has an unique solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cedb44",
   "metadata": {},
   "source": [
    "# Part II (Numerical manipulation)\n",
    "In this part, we consider $\\eta=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ea1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ecdc6c-84c2-4141-98e2-2cd88862234f",
   "metadata": {},
   "source": [
    "**Question 1** Declare the function $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf37c968-0f40-4b6f-8bce-d4b383600247",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ..........."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74935c-49a7-490e-b525-fe01688d0ba0",
   "metadata": {},
   "source": [
    "**Question 2** Let $x \\in [-3,1]\\times [-6,6]$, dewrite the domain using `meshgrid` method and write the function $f$ in its discreet version $F$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3ac022f-ef2c-4995-9496-e43ad422e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = ..........\n",
    "ty = ................\n",
    "[v,u] = np.meshgrid(....)\n",
    "F = ( u ** 2 + eta * v ** 2 ) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f84a96-3752-44f5-a249-4c85525e270f",
   "metadata": {},
   "source": [
    "**Question 3** Use `plt.contourf` to plot the level set of the function $F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb3536",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(......);\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c3cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional : Graph of the function F\n",
    "import plotly.graph_objects as go\n",
    "fig=go.Figure(data=[go.Surface(z=F,x=tx,y=ty)])\n",
    "fig.update_layout( autosize=False,\n",
    "                  width=500, height=500,\n",
    "                  #margin=dict(l=100, r=50, b=65, t=90)\n",
    "                 )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7935e48-f427-4079-92c2-5f0068a3d019",
   "metadata": {},
   "source": [
    "**Question 4** Since the gradient of $f$ is $\\nabla f (x) = (x_0, \\eta x_1)$. Write the function `Gradf` to define the gradient of $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d59cfa-5999-4a1b-b801-3eeb31cdb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradf = lambda ................"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a31315c-b86e-455a-9649-6af8535ea1f4",
   "metadata": {},
   "source": [
    "### Fixed Step Gradient Descent\n",
    "In this exercise, the step size should satisfy $\\tau_k < 2/\\eta$.\n",
    "We use here a constant step size. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360de0cf-a5d5-4a77-8db2-6b37df7a15f8",
   "metadata": {},
   "source": [
    "Write a function `fix_step_dg(f, Gradf,tau, x0, niter)` to perform the gradient descent using a fixed step size $\\tau_k=\\tau$ with $f$ is a given function, `Gradf` is its gradient, `x0`is an intial vector and `niter`is the maximum iteration number. This function should be return the decay of the energy $f(x^{(k)})$ through the iteration and the iterates solutions so that X(:,k) corresponds to $x^{(k)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8870fcb-31d7-4fa3-a96a-8a64778799b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_step_dg(f, Gradf, tau, x0, niter = 20):\n",
    "    x = ...\n",
    "    E = ...\n",
    "    X = .....\n",
    "    for i in ......:\n",
    "        ......\n",
    "        .......\n",
    "    return X, E  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda987b0-894d-449e-b18c-bb72ae589a1f",
   "metadata": {},
   "source": [
    "**Question 5:** \n",
    "**(a)** Test the function for $f$, $x^{(0)} = (0.3, 0.9)$, $\\tau = 1.6/\\eta$ and $niter = 20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42048814-d35d-48e3-94c6-7ff1bad942c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = ......\n",
    "x0 = .......\n",
    "niter = 20\n",
    "X,  E = ..............."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ece25e4-3727-4554-9353-310153f6ef60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.48423766e-05, -1.82807922e-05])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e5175-11a3-4486-9eeb-29dde291ecd6",
   "metadata": {},
   "source": [
    "**(b)** Display the decay of the energy $f(x^{(k)})$ through the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13501a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log10(E))\n",
    "plt.axis('tight')\n",
    "plt.title('$log_{10}f(x^{(k)})$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8caebee-50c2-46e0-b79a-978a78726211",
   "metadata": {},
   "source": [
    "**(c)** Display the iterate solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(.............);\n",
    "plt.colorbar();\n",
    "plt.plot(.........., 'r.-');\n",
    "plt.axis('equal'); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994e541-24f6-4f20-b23e-54a0c5aa061c",
   "metadata": {},
   "source": [
    "**(d)** Display the iteration for several different step sizes `tau_list = np.array([.5, 1, 1.5, 1.9]) / eta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f57f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_list = .....\n",
    "plt.contourf(....);\n",
    "plt.colorbar();\n",
    "for itau in .....:\n",
    "    tau = ....\n",
    "    X = ......\n",
    "    plt.plot(...., '.-')\n",
    "plt.axis('equal') \n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0502edd-d4df-4229-b31e-37a70b205164",
   "metadata": {},
   "source": [
    "### Optimal Step Gradient Descent\n",
    "We use here an optimal step size. \n",
    "\n",
    "**Question (6) (a)**\n",
    "Write a function `optimal_step_dg(f, Gradf, x0, niter)` to perform the gradient descent using an optimal step size $\\tau_k$ with $f$ is a given function, `Gradf` is its gradient, `x0`is an intial vector and `niter`is the maximum iteration number. This function should be return the decay of the energy $f(x^{(k)})$ through the iteration and the iterates solutions so that X(:,k) corresponds to $x^{(k)}$, such that $\\tau_k$ is given by\n",
    "$$\\tau_k=\\frac{\\|d_k\\|^2}{\\langle Ad_k,d_k\\rangle}=\\frac{\\|\\nabla f(x^{(k)})\\|^2}{\\langle A\\nabla f(x^{(k)}),\\nabla f(x^{(k)})\\rangle}$$\n",
    "with $A$ is the matrix associated to $f$ \n",
    "verify that $\\tau_k=\\frac{\\big(x_1^{(k)}\\big)^2+\\eta^2\\big(x_2^{(k)}\\big)^2}{\\big(x_1^{(k)}\\big)^2+\\eta^3\\big(x_2^{(k)}\\big)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e842062e-d61d-40d1-a0c0-d05020f2c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_step_dg(......):\n",
    "    x = ...\n",
    "    E = ....\n",
    "    X = ....\n",
    "    for i in ....:\n",
    "        .....\n",
    "        .....\n",
    "    return X, E "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d54229-c55d-48aa-8c98-6b56ecd1b405",
   "metadata": {},
   "source": [
    "**(b)** Test this function for $f$ and display the iterate solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = .....\n",
    "print('The approximate solution is x = ', ...)\n",
    "\n",
    "# Display the iterate solutions\n",
    "plt.contourf(.....);\n",
    "plt.colorbar();\n",
    "plt.plot(......, 'r.-');\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f47ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
