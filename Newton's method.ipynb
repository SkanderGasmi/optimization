{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc3935d",
   "metadata": {},
   "source": [
    "## Méthode de Newton "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3410c",
   "metadata": {},
   "source": [
    "# Exercice 1: \n",
    " Minimiser $J(x_1,x_2)=x_1-x_2+2x_1^2+2x_1x_2^2+x_2^2$ par l'algorithme de Newton à partir du point $x^0=\\begin{pmatrix}\n",
    "0 \\\\ 0\n",
    "\\end{pmatrix}$? \n",
    "- Effectuer 3 itérations ?  qu est ce que vous remarque ?\n",
    "- Changer le point intiale $x^0=\\begin{pmatrix}\n",
    "1 \\\\ 1\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403366f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dfdad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(x):\n",
    "    return ...............\n",
    "\n",
    "def gradient_J(x):\n",
    "    return .................\n",
    "\n",
    "def hessian_J(x):\n",
    "    return ........................\n",
    "\n",
    "def inverse_hessian_J(x):\n",
    "    return ......................\n",
    "\n",
    "def newton_method(J, gradient_J, hessian_J, inverse_hessian_J, x0, num_iterations):\n",
    "    x = x0\n",
    "    gradients = []\n",
    "    hessians = []\n",
    "    inverse_hessians = []\n",
    "    x_iters = [x]\n",
    "    for i in range(num_iterations):\n",
    "..................\n",
    "..................\n",
    "    return x, x_iters, gradients, hessians, inverse_hessians\n",
    "\n",
    "x0 = np.array([0,0])\n",
    "num_iterations = 1\n",
    "\n",
    "result = newton_method(J, gradient_J, hessian_J, inverse_hessian_J, x0, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Résultat de l'algorithme de Newton :\", result[0])\n",
    "print(\"x_iters :\", result[1])\n",
    "print(\"Gradients :\", result[2])\n",
    "print(\"Hessiens :\", result[3])\n",
    "print(\"Inverses des hessiens :\", result[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot des contours de la fonction J\n",
    "x = np.linspace(-1, 1, 500)\n",
    "y = np.linspace(-1, 1, 500)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = J([X, Y])\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contour(X, Y, Z, levels=20)\n",
    "plt.plot(*zip(*result[1]), '-o', color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Contours de la fonction J avec les itérations de Newton')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664eb98c",
   "metadata": {},
   "source": [
    "# Exercice sue la minimisation d’une fonctionnelle non quadratique\n",
    "# La fonction de Rosenbrock : \n",
    "\n",
    "La fonction de Rosenbrock définie par  $f(x) = 100(x_2-x_1^2)^2 + (x_1-1)^2 $ est une fonction couramment utilisée pour tester les algorithmes de descente de gradient. Sa forme complexe en vallée étroite et allongée avec un minimum global en forme de banane met à l'épreuve la capacité des algorithmes à trouver rapidement le minimum global.\n",
    "La faiblesse du gradient dans de nombreuses régions de l'espace et la sensibilité aux pas de gradient font de la fonction de Rosenbrock un banc d'essai pour évaluer l'efficacité des algorithmes de descente. \n",
    "\n",
    "## Application  de la méthode du gradient et la méthode de Newton\n",
    "\n",
    "Dans cet exercice, nous allons explorer l'optimisation de la fonction de Rosenbrock à l'aide de la méthode du gradient et de la méthode de Newton. La fonction de Rosenbrock est définie comme suit :\n",
    "\n",
    "$$ f(x) = 100(x_2-x_1^2)^2 + (x_1-1)^2 $$\n",
    "## Partie1: Etude Théorique:\n",
    "1-  Trouver les points critiques de $f$.\n",
    "\n",
    "2-Démontrer que $f$ admet un unique minimum global qu’elle atteint en $X^* := (1, 1)$.\n",
    "\n",
    "3-On appelle $H_f$ la matrice hessienne de $f$ calculée en $X^*$. Déterminer $H_f$, puis calculer son conditionnement.\n",
    "\n",
    "4- Expliquer pourquoi ce mauvais conditionnement risque de gêner la convergence des algorithmes numériques. \n",
    "\n",
    "\n",
    "## Partie 2:  Étude Numérique : \n",
    "### Méthode du gradient à pas variable \n",
    "\n",
    "1. Implémentez la fonction `grad_rosenbrock(x)` qui calcule le gradient de la fonction de Rosenbrock en un point donné `x`.\n",
    "\n",
    "2. Implémentez la fonction `Grad_decent(J, nablaJ, X0, beta, alphainit, tau, N)` qui effectue une descente de gradient pour minimiser la fonction `J` avec le gradient `nablaJ`. Les paramètres `X0`, `beta`, `alphainit`, `tau` et `N` représentent respectivement le point de départ, le coefficient de réduction de pas, la taille initiale du pas, le facteur de réduction du pas et le nombre d'itérations.\n",
    "\n",
    "3. Utilisez la fonction `Grad_decent` pour minimiser la fonction de Rosenbrock avec les paramètres suivants :\n",
    "\n",
    "   - Point de départ : `X0 = np.array([0, 1])`\n",
    "   - Coefficient de réduction de pas : `beta = 0.1`\n",
    "   - Taille initiale du pas : `alphainit = 1`\n",
    "   - Facteur de réduction du pas : `tau = 0.3`\n",
    "   - Nombre d'itérations : `N = 10`.\n",
    "\n",
    "4. Affichez la valeur de `X_G` obtenue après 10 itérations. Utilisez la fonction `print` pour afficher les résultats.\n",
    "\n",
    "### Méthode de Newton\n",
    "\n",
    "1. Implémentez la fonction `hessien_rosenbrock(x)` qui calcule la matrice hessienne de la fonction de Rosenbrock en un point donné `x`.\n",
    "\n",
    "2. Implémentez la fonction `New_method(J, nablaJ, HJ, X0, N)` qui effectue une optimisation de Newton pour minimiser la fonction `J` avec le gradient `nablaJ` et la matrice hessienne `HJ`. Les paramètres `X0` et `N` représentent respectivement le point de départ et le nombre d'itérations.\n",
    "\n",
    "3. Utilisez la fonction `New_method` pour minimiser la fonction de Rosenbrock avec les paramètres suivants :\n",
    "\n",
    "   - Point de départ : `X0 = np.array([0, 1])`\n",
    "   - Nombre d'itérations : `N = 10`\n",
    "\n",
    "4. Affichez la valeur de `X_N` obtenue après 10 itérations. Utilisez la fonction `print` pour afficher les résultats.\n",
    "\n",
    "### Visualisation des résultats\n",
    "\n",
    "1. Construisez une représentation graphique en 3D de la fonction de Rosenbrock. Utilisez la bibliothèque `matplotlib` pour créer le graphique.\n",
    "\n",
    "2. Tracez les contours de la fonction de Rosenbrock avec les itérations du gradient et de Newton. Utilisez la fonction `contour` de `matplotlib` pour tracer les contours de la fonction et les fonctions `plot` pour afficher les itérations du gradient et de Newton.\n",
    "\n",
    "3. Ajoutez une annotation pour indiquer le point de départ sur le graphique.\n",
    "\n",
    "4. Affichez les courbes logarithmiques des valeurs de la fonction de Rosenbrock pour les itérations du gradient et de Newton. Utilisez la fonction `plot` de `matplotlib` pour tracer les courbes.\n",
    "\n",
    "5. Affichez le graphique final.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d688798d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc458db5",
   "metadata": {},
   "source": [
    "# Partie 2: Étude Numérique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de Rosenbrock\n",
    "def rosenbrock(x):\n",
    "    return ...................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80040733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partie 1: Méthode du gradient\n",
    "def grad_rosenbrock(x):\n",
    "    return .................................\n",
    "\n",
    "def Grad_Desent(J, nablaJ, X0, beta, alphainit, tau, N):\n",
    "    X = X0\n",
    "    X_iters = [X]\n",
    "    alpha = alphainit\n",
    "    for i in range(N):\n",
    ".............\n",
    "............\n",
    "..............\n",
    "    return X, X_iters\n",
    "\n",
    "X0 = np.array([0, 1])\n",
    "beta = 0.1\n",
    "alphainit = 1\n",
    "tau = 0.3\n",
    "N = 10\n",
    "X_G = Grad_Desent(rosenbrock, grad_rosenbrock, X0, beta, alphainit, tau, N)[1]\n",
    "print(\"Résultat de la méthode du gradient : \", X_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc6c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partie 2: Méthode de Newton\n",
    "def hessien_rosenbrock(x):\n",
    "    return ...........................\n",
    "\n",
    "def New_method(J, nablaJ, HJ, X0, N):\n",
    "    X = X0\n",
    "    x_iters = [X]\n",
    "    for i in range(N):\n",
    "..........\n",
    "...........\n",
    "    return X, x_iters\n",
    "\n",
    "N = 10\n",
    "X_N = New_method(rosenbrock, grad_rosenbrock, hessien_rosenbrock, X0, N)[1]\n",
    "print(\"Résultat de la méthode de Newton : \", X_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Représentation graphique\n",
    "# Grille de points pour le graphique de la fonction de Rosenbrock\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-1, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = rosenbrock([X, Y])\n",
    "\n",
    "# Graphique 3D de la fonction de Rosenbrock\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('Rosenbrock(x, y)')\n",
    "ax.set_title('Fonction de Rosenbrock en 3D')\n",
    "\n",
    "# Graphique des contours de la fonction de Rosenbrock avec les itérations du gradient et de Newton\n",
    "plt.figure()\n",
    "plt.contour(X, Y, Z, levels=50, cmap='viridis')\n",
    "plt.plot([x[0] for x in X_G], [x[1] for x in X_G], 'b*-')\n",
    "plt.plot([x[0] for x in X_N], [x[1] for x in X_N], 'r*-')\n",
    "plt.annotate('Départ', xy=(X0[0], X0[1]), xytext=(-1, 1.5),\n",
    "             arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Contours de la fonction de Rosenbrock avec les itérations du gradient et de Newton')\n",
    "\n",
    "# Graphique des courbes logarithmiques des valeurs de la fonction de Rosenbrock\n",
    "plt.figure()\n",
    "Jval_G = [rosenbrock(x) for x in X_G]\n",
    "Jval_N = [rosenbrock(x) for x in X_N]\n",
    "plt.semilogy(range(N+1), Jval_G, 'b*-', label='Gradient')\n",
    "plt.semilogy(range(N+1), Jval_N, 'r*-', label='Newton')\n",
    "plt.xlabel('Itération')\n",
    "plt.ylabel('Valeur de la fonction (log)')\n",
    "plt.title('Courbes logarithmiques des valeurs de la fonction de Rosenbrock')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37303f0",
   "metadata": {},
   "source": [
    "Le  code precedent  utilise une boucle for avec un nombre fixe d'itérations prédéfinies pour les méthodes du gradient et de Newton. En revanche, le deuxième code utilise une boucle while avec un critère d'arrêt basé sur la norme du gradient. Cela signifie que la boucle se poursuivra tant que la norme du gradient est supérieure à une certaine valeur seuil (epsilon) ou que le nombre d'itérations maximum est atteint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partie 1: Méthode du gradient\n",
    "def Grad_Desent(J, nablaJ, X0, beta, alphainit, tau, max_iter, epsilon):\n",
    "    X = X0\n",
    "    X_iters = [X]\n",
    "    alpha = alphainit\n",
    "    i = 0\n",
    "    while .....................:\n",
    "        X_new = ...................\n",
    "        if J(X_new) < J(X):\n",
    "            X = X_new\n",
    "            alpha *= beta\n",
    "        else:\n",
    "            alpha *= tau\n",
    "        X_iters.append(X)\n",
    "        i += 1\n",
    "    return X, X_iters\n",
    "\n",
    "X0 = np.array([0, 1])\n",
    "beta = 0.1\n",
    "alphainit = 1\n",
    "tau = 0.3\n",
    "max_iter = 1000\n",
    "epsilon = 1e-1\n",
    "X_G = Grad_Desent(rosenbrock, grad_rosenbrock, X0, beta, alphainit, tau, max_iter, epsilon)[1]\n",
    "#print(\"Résultat de la méthode du gradient : \", X_G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partie 2: Méthode de Newton\n",
    "\n",
    "def New_method(J, nablaJ, HJ, X0, max_iter, epsilon):\n",
    "    X = X0\n",
    "    x_iters = [X]\n",
    "    i = 0\n",
    "    while i < max_iter and np.linalg.norm(nablaJ(X)) > epsilon:\n",
    "        X_new = X - np.linalg.inv(HJ(X)).dot(nablaJ(X))\n",
    "        X = X_new\n",
    "        x_iters.append(X)\n",
    "        i += 1\n",
    "    return X, x_iters\n",
    "\n",
    "X_N = New_method(rosenbrock, grad_rosenbrock, hessien_rosenbrock, X0, max_iter, epsilon)[1]\n",
    "#print(\"Résultat de la méthode de Newton : \", X_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Représentation graphique\n",
    "\n",
    "# Grille de points pour le graphique de la fonction de Rosenbrock\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-1, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = rosenbrock([X, Y])\n",
    "\n",
    "# Graphique 3D de la fonction de Rosenbrock\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('Rosenbrock(x, y)')\n",
    "ax.set_title('Fonction de Rosenbrock en 3D')\n",
    "\n",
    "# Graphique des contours de la fonction de Rosenbrock avec les itérations du gradient et de Newton\n",
    "plt.figure()\n",
    "plt.contour(X, Y, Z, levels=50, cmap='viridis')\n",
    "plt.plot([x[0] for x in X_G], [x[1] for x in X_G], 'b*-')\n",
    "plt.plot([x[0] for x in X_N], [x[1] for x in X_N], 'r*-')\n",
    "plt.annotate('Départ', xy=(X0[0], X0[1]), xytext=(-1, 1.5),\n",
    "             arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Contours de la fonction de Rosenbrock avec les itérations du gradient et de Newton')\n",
    "\n",
    "# Graphique des courbes logarithmiques des valeurs de la fonction de Rosenbrock\n",
    "plt.figure()\n",
    "Jval_G = [rosenbrock(x) for x in X_G]\n",
    "Jval_N = [rosenbrock(x) for x in X_N]\n",
    "plt.semilogy(range(len(X_G)), Jval_G, 'b*-', label='Gradient')\n",
    "plt.semilogy(range(len(X_N)), Jval_N, 'r*-', label='Newton')\n",
    "plt.xlabel('Itération')\n",
    "plt.ylabel('Valeur de la fonction (log)')\n",
    "plt.title('Courbes logarithmiques des valeurs de la fonction de Rosenbrock')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
